\documentclass[11pt]{article}

\usepackage{geometry}
\usepackage{amsmath, amsthm, amssymb, bbm}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{xcolor}
\usepackage{natbib}
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing,calc}

% Define colors for emphasis
\definecolor{mAlert}{RGB}{235, 129, 27}
\newcommand{\alert}[1]{\textcolor{mAlert}{#1}}

\newtheorem{assumption}{Assumption}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}

\title{AI, Headquarters and Guijie}
\author{Zehao Zhang}
% \date{\today}

\setstretch{1.1}

\begin{document}

\maketitle

\section*{Simplified attribution model}

\subsection*{Environment and timing}
A principal (headquarters, \(P\)) contracts with an agent (local manager, \(A\)). The agent's ability \(\theta\in\{\theta_L,\theta_H\}\) satisfies \(\theta_H>\theta_L\) and is privately known to \(A\). The prior distribution of \(\theta\), \(\Pr(\theta = \theta_H) = \mu \in (0,1)\), is common knowledge.

Timeline: (1) \(P\) offers a linear contract \(w(Y)=a+bY\) with \(a\ge 0\). (2) \(A\) chooses effort \(e\ge 0\). (3) Outcome \(Y\in\{0,1\}\) realizes and pay is made.

\subsection*{Technology and attribution}
Success requires both the agent's action and a favorable external state. Let \(A_e\sim\mathrm{Bernoulli}(\theta e)\) be agent-driven success and \(L\sim\mathrm{Bernoulli}(\lambda)\) be exogenous luck, independent. The observed success is the conjunction
\[
Y \,=\, \min\{A_e,\,L\}\quad\Longrightarrow\quad \Pr(Y=1\mid \theta,e) \,=\, \lambda\,\theta e.
\]
This encodes attribution stringency: success is creditable only when both the agent performs and luck realizes.

\subsection*{Preferences and objectives}
Players are risk-neutral. The agent's cost of effort is \(c(e,\theta)=e^2/(2\theta)\) (effort is cheaper for higher ability). The principal values a success at \(v>0\).

\subsection*{Agent problem and best response}
Given \((a,b)\), the agent maximizes
\[
\max_{e\ge 0}\; a + b\,\big[\lambda\,\theta e\big] - \frac{e^2}{2\theta}.
\]
For interior solutions, the FOC yields
\[
 e^*(\theta;b,\lambda) \,=\, b\,\lambda\,\theta^2,\qquad \frac{\partial e^*}{\partial \lambda}=b\,\theta^2>0,\; \frac{\partial e^*}{\partial \theta}=2b\,\lambda\,\theta>0.
\]
The induced success probability is
\[
 p(\theta;b,\lambda)\equiv \Pr(Y=1\mid \theta,e^*) \,=\, \lambda\,\theta\,e^* \,=\, b\,\lambda^2\,\theta^3.
\]

\subsection*{Propositions}
\begin{proposition}[Incentive amplification under necessary luck]
The marginal impact of effort on success is \(\partial\Pr(Y=1)/\partial e = \lambda\,\theta\). Hence, for any fixed bonus \(b\), the agent's optimal effort \(e^*(\theta;b,\lambda)=b\,\lambda\,\theta^2\) is strictly increasing in \(\lambda\). A more favorable environment (higher \(\lambda\)) strengthens performance-based incentives and raises effort.
\end{proposition}

\begin{proposition}[Attribution stringency strengthens screening]
Under any common bonus \(b\), the success probability by type satisfies \(p(\theta_H;b,\lambda)-p(\theta_L;b,\lambda)=b\,\lambda^2\,\big(\theta_H^3-\theta_L^3\big)>0\) and is strictly increasing in \(\lambda\). When success requires both luck and effort, improvements in the external environment (higher \(\lambda\)) enlarge type separation in outcomes and strengthen the screening power of performance pay.
\end{proposition}

\subsection*{Remarks}
\begin{itemize}
\item All incentive and screening forces scale with \(\lambda\): when \(\lambda\to 0\), effort becomes ineffective and performance pay loses power; when \(\lambda\to 1\), returns to performance pay are maximal.
\item Additional signals or contract instruments that separately observe effort or the external state would further sharpen attribution; absent such instruments, \(\lambda\) is the sufficient statistic for incentive and screening strength.
\end{itemize}

\end{document}